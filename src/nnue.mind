// NNUE Neural Network Evaluation - Pure Mind Implementation
// GPU-accelerated via MIND Runtime

use std::simd;
use runtime::tensor;
use runtime::cuda;

// HalfKA feature dimensions
const HALF_DIMS: usize = 40960;  // 64 * 640 (king square * piece features)
const L1_SIZE: usize = 1024;
const L2_SIZE: usize = 16;
const L3_SIZE: usize = 32;

// Network weights structure
struct NNUEWeights {
    // Feature transformer
    ft_weights: tensor<i16, (HALF_DIMS, L1_SIZE)>,
    ft_biases: tensor<i16, (L1_SIZE,)>,

    // Hidden layers
    l1_weights: tensor<i8, (L1_SIZE * 2, L2_SIZE)>,
    l1_biases: tensor<i32, (L2_SIZE,)>,

    l2_weights: tensor<i8, (L2_SIZE, L3_SIZE)>,
    l2_biases: tensor<i32, (L3_SIZE,)>,

    // Output layer with WDL head
    output_weights: tensor<i8, (L3_SIZE, 3)>,
    output_biases: tensor<i32, (3,)>,
}

// Accumulator for incremental updates
struct Accumulator {
    white: tensor<i16, (L1_SIZE,)>,
    black: tensor<i16, (L1_SIZE,)>,
    computed: bool,
}

impl Accumulator {
    fn new() -> Self {
        Accumulator {
            white: tensor::zeros(),
            black: tensor::zeros(),
            computed: false,
        }
    }

    fn refresh(&mut self, weights: &NNUEWeights, features: &[u16]) {
        // Reset to biases
        self.white = weights.ft_biases.clone();
        self.black = weights.ft_biases.clone();

        // Add active features using SIMD
        simd::parallel_for(features) |idx| {
            let white_idx = idx as usize;
            let black_idx = flip_feature(idx) as usize;

            self.white += weights.ft_weights[white_idx];
            self.black += weights.ft_weights[black_idx];
        }

        self.computed = true;
    }

    fn update_add(&mut self, weights: &NNUEWeights, feature: u16) {
        let white_idx = feature as usize;
        let black_idx = flip_feature(feature) as usize;

        self.white += weights.ft_weights[white_idx];
        self.black += weights.ft_weights[black_idx];
    }

    fn update_sub(&mut self, weights: &NNUEWeights, feature: u16) {
        let white_idx = feature as usize;
        let black_idx = flip_feature(feature) as usize;

        self.white -= weights.ft_weights[white_idx];
        self.black -= weights.ft_weights[black_idx];
    }
}

// SCReLU activation (Squared Clipped ReLU)
#[inline]
fn screlu(x: i16) -> i32 {
    let clipped = x.clamp(0, 127) as i32;
    clipped * clipped
}

// Forward pass through the network
fn forward(acc: &Accumulator, weights: &NNUEWeights, stm: Color) -> i32 {
    // Get perspectives based on side to move
    let (us, them) = if stm == Color::White {
        (&acc.white, &acc.black)
    } else {
        (&acc.black, &acc.white)
    };

    // L1: Apply SCReLU and concatenate perspectives
    let mut l1_out: tensor<i32, (L1_SIZE * 2,)> = tensor::zeros();

    simd::parallel_for(0..L1_SIZE) |i| {
        l1_out[i] = screlu(us[i]);
        l1_out[i + L1_SIZE] = screlu(them[i]);
    }

    // L2: Matrix multiply + bias
    let l2_input = tensor::matmul(l1_out, weights.l1_weights);
    let mut l2_out: tensor<i32, (L2_SIZE,)> = tensor::zeros();

    simd::parallel_for(0..L2_SIZE) |i| {
        let val = (l2_input[i] + weights.l1_biases[i]) >> 6;
        l2_out[i] = screlu(val.clamp(-32768, 32767) as i16);
    }

    // L3: Matrix multiply + bias
    let l3_input = tensor::matmul(l2_out, weights.l2_weights);
    let mut l3_out: tensor<i32, (L3_SIZE,)> = tensor::zeros();

    simd::parallel_for(0..L3_SIZE) |i| {
        let val = (l3_input[i] + weights.l2_biases[i]) >> 6;
        l3_out[i] = screlu(val.clamp(-32768, 32767) as i16);
    }

    // Output: WDL head
    let output = tensor::matmul(l3_out, weights.output_weights);
    let win = output[0] + weights.output_biases[0];
    let draw = output[1] + weights.output_biases[1];
    let loss = output[2] + weights.output_biases[2];

    // Convert WDL to centipawns
    wdl_to_cp(win, draw, loss)
}

// GPU batch evaluation
fn batch_eval(
    positions: &[Board],
    weights: &NNUEWeights,
    results: &mut [i32]
) {
    on(cuda::gpu0) {
        parallel for i in 0..positions.len() {
            let board = &positions[i];
            let features = extract_features(board);

            let mut acc = Accumulator::new();
            acc.refresh(weights, &features);

            results[i] = forward(&acc, weights, board.side_to_move());
        }
    }
}

// Feature extraction helpers
fn extract_features(board: &Board) -> Vec<u16> {
    let mut features = Vec::with_capacity(32);
    let white_king = board.king_square(Color::White);
    let black_king = board.king_square(Color::Black);

    for sq in 0..64 {
        if let Some(piece) = board.piece_on(sq) {
            let white_feature = halfka_index(white_king, sq, piece);
            let black_feature = halfka_index(black_king.flip(), sq.flip(), piece.flip());
            features.push(white_feature);
            features.push(black_feature);
        }
    }

    features
}

fn halfka_index(king_sq: Square, piece_sq: Square, piece: Piece) -> u16 {
    let piece_type = piece.piece_type() as u16;
    let color = piece.color() as u16;
    let piece_idx = piece_type * 2 + color;

    (king_sq as u16) * 640 + (piece_sq as u16) * 10 + piece_idx
}

fn flip_feature(feature: u16) -> u16 {
    // Flip perspective for black
    let king_sq = feature / 640;
    let rest = feature % 640;
    let piece_sq = rest / 10;
    let piece_idx = rest % 10;

    // Flip squares vertically
    let flipped_king = king_sq ^ 56;
    let flipped_piece = piece_sq ^ 56;
    // Flip piece color
    let flipped_idx = piece_idx ^ 1;

    flipped_king * 640 + flipped_piece * 10 + flipped_idx
}

fn wdl_to_cp(win: i32, draw: i32, loss: i32) -> i32 {
    // Softmax normalization
    let total = win + draw + loss;
    if total == 0 {
        return 0;
    }

    let win_prob = (win * 1000) / total;
    let loss_prob = (loss * 1000) / total;

    // Convert to centipawns using logistic curve
    let expected = win_prob - loss_prob;
    (expected * 400) / 1000
}

// Load weights from .nknn/.nnkn file with auto-detection
pub fn load_weights(path: &str) -> Result<NNUEWeights, Error> {
    let file = std::fs::read(path)?;
    let mut cursor = 0;

    // Parse header - support both NKNN and NNKN magic bytes
    let magic = read_u32(&file, &mut cursor);
    let format_type = match magic {
        0x4E4B4E4E => "NKNN",  // Legacy weights format
        0x4E4E4B4E => "NNKN",  // New trained model format
        _ => return Err(Error::InvalidFormat),
    };

    println!("Detected {} format", format_type);

    let version = read_u32(&file, &mut cursor);

    // Version selector - automatically detect and load appropriate format
    match (format_type, version) {
        ("NKNN", 1) => load_weights_v1(&file, &mut cursor),
        ("NKNN", 2) => load_weights_v2(&file, &mut cursor),
        ("NNKN", 1) => load_weights_nnkn_v1(&file, &mut cursor),
        ("NNKN", 2) => load_weights_nnkn_v2(&file, &mut cursor),
        _ => Err(Error::UnsupportedVersion(version)),
    }
}

// v1 format: HalfKP, 40960 features, 256 L1, single hidden layer
fn load_weights_v1(file: &[u8], cursor: &mut usize) -> Result<NNUEWeights, Error> {
    println!("Loading NNUE v1 format (HalfKP, legacy)");

    // v1 header: architecture (16 bytes), feature_size, l1_size
    let _arch = read_bytes(file, cursor, 16);
    let feature_size = read_u32(file, cursor) as usize;  // 40960
    let l1_size = read_u32(file, cursor) as usize;       // 256

    // Read v1 weights and convert to v2 structure
    let ft_weights_v1: tensor<i16, (40960, 256)> = read_tensor(file, cursor);
    let ft_biases_v1: tensor<i16, (256,)> = read_tensor(file, cursor);
    let l1_weights_v1: tensor<i8, (512, 32)> = read_tensor(file, cursor);
    let l1_biases_v1: tensor<i32, (32,)> = read_tensor(file, cursor);
    let output_weights_v1: tensor<i8, (32, 1)> = read_tensor(file, cursor);
    let output_bias_v1: i32 = read_i32(file, cursor);

    // Convert to v2 structure with padding/expansion
    let weights = NNUEWeights {
        ft_weights: expand_ft_weights(ft_weights_v1),      // 40960x256 -> 40960x1024
        ft_biases: expand_ft_biases(ft_biases_v1),         // 256 -> 1024
        l1_weights: convert_l1_weights(l1_weights_v1),     // 512x32 -> 2048x16
        l1_biases: convert_l1_biases(l1_biases_v1),        // 32 -> 16
        l2_weights: identity_l2(),                          // Identity for missing layer
        l2_biases: tensor::zeros(),
        output_weights: expand_output(output_weights_v1),   // 32x1 -> 32x3 (WDL)
        output_biases: tensor::from_array([output_bias_v1, 0, 0]),
    };

    Ok(weights)
}

// v2 format: HalfKA, 45056 features, 1024 L1, two hidden layers, WDL head
fn load_weights_v2(file: &[u8], cursor: &mut usize) -> Result<NNUEWeights, Error> {
    println!("Loading NNUE v2 format (HalfKA, current)");

    // v2 header: architecture, sizes, quantization, flags
    let _arch = read_bytes(file, cursor, 16);
    let _feature_size = read_u32(file, cursor);  // 45056
    let _l1_size = read_u32(file, cursor);       // 1024
    let _l2_size = read_u32(file, cursor);       // 16
    let _l3_size = read_u32(file, cursor);       // 32
    let _quant = read_bytes(file, cursor, 8);
    let _flags = read_u32(file, cursor);
    let _reserved = read_bytes(file, cursor, 12);

    // Read v2 weights directly
    let weights = NNUEWeights {
        ft_weights: read_tensor(file, cursor),
        ft_biases: read_tensor(file, cursor),
        l1_weights: read_tensor(file, cursor),
        l1_biases: read_tensor(file, cursor),
        l2_weights: read_tensor(file, cursor),
        l2_biases: read_tensor(file, cursor),
        output_weights: read_tensor(file, cursor),
        output_biases: read_tensor(file, cursor),
    };

    Ok(weights)
}

// Helper functions for v1 -> v2 conversion
fn expand_ft_weights(v1: tensor<i16, (40960, 256)>) -> tensor<i16, (HALF_DIMS, L1_SIZE)> {
    let mut out = tensor::zeros();
    for i in 0..40960 {
        for j in 0..256 {
            out[i][j] = v1[i][j];
        }
    }
    out
}

fn expand_ft_biases(v1: tensor<i16, (256,)>) -> tensor<i16, (L1_SIZE,)> {
    let mut out = tensor::zeros();
    for i in 0..256 {
        out[i] = v1[i];
    }
    out
}

fn convert_l1_weights(v1: tensor<i8, (512, 32)>) -> tensor<i8, (L1_SIZE * 2, L2_SIZE)> {
    let mut out = tensor::zeros();
    for i in 0..512 {
        for j in 0..16.min(32) {
            out[i][j] = v1[i][j];
        }
    }
    out
}

fn convert_l1_biases(v1: tensor<i32, (32,)>) -> tensor<i32, (L2_SIZE,)> {
    let mut out = tensor::zeros();
    for i in 0..16 {
        out[i] = v1[i];
    }
    out
}

fn identity_l2() -> tensor<i8, (L2_SIZE, L3_SIZE)> {
    let mut out = tensor::zeros();
    for i in 0..L2_SIZE.min(L3_SIZE) {
        out[i][i] = 64;  // Identity with scale
    }
    out
}

fn expand_output(v1: tensor<i8, (32, 1)>) -> tensor<i8, (L3_SIZE, 3)> {
    let mut out = tensor::zeros();
    for i in 0..32 {
        out[i][0] = v1[i][0];  // Win channel
        out[i][2] = -v1[i][0]; // Loss channel (negated)
    }
    out
}

// ============================================================================
// NNKN FORMAT LOADERS (Trained model format from consensus)
// ============================================================================

// NNKN v1: Basic trained weights format
fn load_weights_nnkn_v1(file: &[u8], cursor: &mut usize) -> Result<NNUEWeights, Error> {
    println!("Loading NNKN v1 format (basic trained model)");

    // NNKN v1 header: record_count (8 bytes), input_size, hidden_size
    let _record_count = read_u64(file, cursor);
    let input_size = read_u32(file, cursor) as usize;
    let hidden_size = read_u32(file, cursor) as usize;

    println!("  Input size: {}, Hidden size: {}", input_size, hidden_size);

    // Read weights in NNKN format (f32 -> convert to quantized)
    let ft_weights_f32: Vec<f32> = read_floats(file, cursor, input_size * hidden_size);
    let ft_biases_f32: Vec<f32> = read_floats(file, cursor, hidden_size);

    // Quantize to internal format
    let weights = quantize_nnkn_to_internal(
        ft_weights_f32, ft_biases_f32,
        input_size, hidden_size
    );

    Ok(weights)
}

// NNKN v2: Extended format with WDL and multiple layers
fn load_weights_nnkn_v2(file: &[u8], cursor: &mut usize) -> Result<NNUEWeights, Error> {
    println!("Loading NNKN v2 format (extended trained model with WDL)");

    // NNKN v2 extended header
    let record_count = read_u64(file, cursor);
    let input_size = read_u32(file, cursor) as usize;   // 40960
    let l1_size = read_u32(file, cursor) as usize;      // 256 or 1024
    let l2_size = read_u32(file, cursor) as usize;      // 32
    let l3_size = read_u32(file, cursor) as usize;      // 32
    let has_wdl = read_u32(file, cursor) != 0;

    println!("  Records: {}, Input: {}, L1: {}, L2: {}, L3: {}, WDL: {}",
             record_count, input_size, l1_size, l2_size, l3_size, has_wdl);

    // Skip reserved header bytes
    let _reserved = read_bytes(file, cursor, 16);

    // Check if this is weights or training data
    if record_count > 1000 {
        // This is training data, not weights - cannot load directly
        return Err(Error::InvalidFormat("NNKN v2 contains training data, not weights"));
    }

    // Read quantization parameters
    let quant_scale_ft = read_f32(file, cursor);
    let quant_scale_dense = read_f32(file, cursor);

    // Read feature transformer
    let ft_weights = read_tensor_i16(file, cursor, input_size, l1_size);
    let ft_biases = read_tensor_i16_1d(file, cursor, l1_size);

    // Read hidden layers
    let l1_weights = read_tensor_i8(file, cursor, l1_size * 2, l2_size);
    let l1_biases = read_tensor_i32_1d(file, cursor, l2_size);

    let l2_weights = read_tensor_i8(file, cursor, l2_size, l3_size);
    let l2_biases = read_tensor_i32_1d(file, cursor, l3_size);

    // Read output layer (WDL: 3 outputs, or scalar: 1 output)
    let out_size = if has_wdl { 3 } else { 1 };
    let output_weights = read_tensor_i8(file, cursor, l3_size, out_size);
    let output_biases = read_tensor_i32_1d(file, cursor, out_size);

    // Expand to internal format if needed
    let weights = NNUEWeights {
        ft_weights: expand_to_internal_ft(ft_weights, input_size, l1_size),
        ft_biases: expand_to_internal_bias(ft_biases, l1_size),
        l1_weights: expand_to_internal_l1(l1_weights, l1_size * 2, l2_size),
        l1_biases: expand_to_internal_l1_bias(l1_biases, l2_size),
        l2_weights: expand_to_internal_l2(l2_weights, l2_size, l3_size),
        l2_biases: expand_to_internal_l2_bias(l2_biases, l3_size),
        output_weights: expand_to_internal_out(output_weights, l3_size, out_size, has_wdl),
        output_biases: expand_to_internal_out_bias(output_biases, out_size, has_wdl),
    };

    Ok(weights)
}

// Helper: Read f32 array
fn read_floats(file: &[u8], cursor: &mut usize, count: usize) -> Vec<f32> {
    let mut result = Vec::with_capacity(count);
    for _ in 0..count {
        result.push(read_f32(file, cursor));
    }
    result
}

// Helper: Quantize f32 weights to internal format
fn quantize_nnkn_to_internal(
    ft_weights: Vec<f32>, ft_biases: Vec<f32>,
    input_size: usize, hidden_size: usize
) -> NNUEWeights {
    const QUANT_SCALE: f32 = 64.0;

    // Convert and expand to internal dimensions
    let mut weights = NNUEWeights {
        ft_weights: tensor::zeros(),
        ft_biases: tensor::zeros(),
        l1_weights: tensor::zeros(),
        l1_biases: tensor::zeros(),
        l2_weights: identity_l2(),
        l2_biases: tensor::zeros(),
        output_weights: tensor::zeros(),
        output_biases: tensor::zeros(),
    };

    // Quantize feature transformer
    let in_dim = input_size.min(HALF_DIMS);
    let hid_dim = hidden_size.min(L1_SIZE);

    for i in 0..in_dim {
        for j in 0..hid_dim {
            let idx = i * hidden_size + j;
            if idx < ft_weights.len() {
                weights.ft_weights[i][j] = (ft_weights[idx] * QUANT_SCALE).clamp(-32768.0, 32767.0) as i16;
            }
        }
    }

    for j in 0..hid_dim {
        if j < ft_biases.len() {
            weights.ft_biases[j] = (ft_biases[j] * QUANT_SCALE).clamp(-32768.0, 32767.0) as i16;
        }
    }

    weights
}

// Expansion helpers for NNKN -> internal format
fn expand_to_internal_ft(src: tensor<i16, dyn>, in_size: usize, hid_size: usize) -> tensor<i16, (HALF_DIMS, L1_SIZE)> {
    let mut out = tensor::zeros();
    for i in 0..in_size.min(HALF_DIMS) {
        for j in 0..hid_size.min(L1_SIZE) {
            out[i][j] = src[i][j];
        }
    }
    out
}

fn expand_to_internal_bias(src: tensor<i16, dyn>, size: usize) -> tensor<i16, (L1_SIZE,)> {
    let mut out = tensor::zeros();
    for i in 0..size.min(L1_SIZE) {
        out[i] = src[i];
    }
    out
}

fn expand_to_internal_l1(src: tensor<i8, dyn>, in_size: usize, out_size: usize) -> tensor<i8, (L1_SIZE * 2, L2_SIZE)> {
    let mut out = tensor::zeros();
    for i in 0..in_size.min(L1_SIZE * 2) {
        for j in 0..out_size.min(L2_SIZE) {
            out[i][j] = src[i][j];
        }
    }
    out
}

fn expand_to_internal_l1_bias(src: tensor<i32, dyn>, size: usize) -> tensor<i32, (L2_SIZE,)> {
    let mut out = tensor::zeros();
    for i in 0..size.min(L2_SIZE) {
        out[i] = src[i];
    }
    out
}

fn expand_to_internal_l2(src: tensor<i8, dyn>, in_size: usize, out_size: usize) -> tensor<i8, (L2_SIZE, L3_SIZE)> {
    let mut out = tensor::zeros();
    for i in 0..in_size.min(L2_SIZE) {
        for j in 0..out_size.min(L3_SIZE) {
            out[i][j] = src[i][j];
        }
    }
    out
}

fn expand_to_internal_l2_bias(src: tensor<i32, dyn>, size: usize) -> tensor<i32, (L3_SIZE,)> {
    let mut out = tensor::zeros();
    for i in 0..size.min(L3_SIZE) {
        out[i] = src[i];
    }
    out
}

fn expand_to_internal_out(src: tensor<i8, dyn>, in_size: usize, out_size: usize, has_wdl: bool) -> tensor<i8, (L3_SIZE, 3)> {
    let mut out = tensor::zeros();
    for i in 0..in_size.min(L3_SIZE) {
        if has_wdl {
            for j in 0..out_size.min(3) {
                out[i][j] = src[i][j];
            }
        } else {
            // Single output -> expand to WDL
            out[i][0] = src[i][0];   // Win
            out[i][1] = 0;           // Draw
            out[i][2] = -src[i][0];  // Loss (negated)
        }
    }
    out
}

fn expand_to_internal_out_bias(src: tensor<i32, dyn>, size: usize, has_wdl: bool) -> tensor<i32, (3,)> {
    let mut out = tensor::zeros();
    if has_wdl && size >= 3 {
        out[0] = src[0];
        out[1] = src[1];
        out[2] = src[2];
    } else if size >= 1 {
        out[0] = src[0];
        out[1] = 0;
        out[2] = -src[0];
    }
    out
}
