// NikolaChess - Transformer Head for Root Move Reranking
// Copyright (c) 2026 STARGA, Inc. All rights reserved.
// PROPRIETARY AND CONFIDENTIAL
//
// Uses lightweight 2-layer transformer to rerank root moves
// Solves "Horizon Effect" better than pure NNUE
// Expected ELO gain: +15-30

import std.tensor;
import std.cuda;
import std.math;

// ============================================================================
// CONFIGURATION
// ============================================================================

const D_MODEL: usize = 256;      // Embedding dimension
const N_HEADS: usize = 4;        // Number of attention heads
const D_HEAD: usize = 64;        // Dimension per head (D_MODEL / N_HEADS)
const D_FF: usize = 512;         // Feed-forward hidden dimension
const N_LAYERS: usize = 2;       // Number of transformer layers
const MAX_MOVES: usize = 64;     // Maximum moves to consider

// ============================================================================
// ATTENTION HEAD
// ============================================================================

struct AttentionHead {
    w_q: Tensor[f32, (D_MODEL, D_HEAD)],  // Query projection
    w_k: Tensor[f32, (D_MODEL, D_HEAD)],  // Key projection
    w_v: Tensor[f32, (D_MODEL, D_HEAD)],  // Value projection
}

fn create_attention_head() -> AttentionHead {
    let scale = (2.0 / D_MODEL as f32).sqrt();
    return AttentionHead {
        w_q: random_tensor((D_MODEL, D_HEAD), scale),
        w_k: random_tensor((D_MODEL, D_HEAD), scale),
        w_v: random_tensor((D_MODEL, D_HEAD), scale),
    };
}

// ============================================================================
// MULTI-HEAD ATTENTION
// ============================================================================

struct MultiHeadAttention {
    heads: Vec<AttentionHead>,
    w_o: Tensor[f32, (N_HEADS * D_HEAD, D_MODEL)],  // Output projection
}

fn create_mha() -> MultiHeadAttention {
    let mut heads = Vec.new();
    for _ in 0..N_HEADS {
        heads.push(create_attention_head());
    }

    let scale = (2.0 / (N_HEADS * D_HEAD) as f32).sqrt();
    return MultiHeadAttention {
        heads: heads,
        w_o: random_tensor((N_HEADS * D_HEAD, D_MODEL), scale),
    };
}

fn attention_forward(
    mha: &MultiHeadAttention,
    input: &Vec<Vec<f32>>  // [num_moves, D_MODEL]
) -> Vec<Vec<f32>> {
    let num_moves = input.len();
    let mut all_head_outputs = vec![vec![0.0f32; N_HEADS * D_HEAD]; num_moves];

    // Process each head
    for (head_idx, head) in mha.heads.iter().enumerate() {
        // Compute Q, K, V for all positions
        let queries = input.iter().map(|x| linear(x, &head.w_q)).collect::<Vec<_>>();
        let keys = input.iter().map(|x| linear(x, &head.w_k)).collect::<Vec<_>>();
        let values = input.iter().map(|x| linear(x, &head.w_v)).collect::<Vec<_>>();

        let scale = 1.0 / (D_HEAD as f32).sqrt();

        // Compute attention for each position
        for i in 0..num_moves {
            // Attention weights
            let mut attn_weights = Vec.new();
            for k in &keys {
                attn_weights.push(dot_product(&queries[i], k) * scale);
            }

            // Softmax
            softmax_inplace(&mut attn_weights);

            // Weighted sum of values
            let mut output = vec![0.0f32; D_HEAD];
            for (j, v) in values.iter().enumerate() {
                for k in 0..D_HEAD {
                    output[k] += attn_weights[j] * v[k];
                }
            }

            // Store in head output
            for k in 0..D_HEAD {
                all_head_outputs[i][head_idx * D_HEAD + k] = output[k];
            }
        }
    }

    // Output projection
    return all_head_outputs.iter()
        .map(|h| linear(h, &mha.w_o))
        .collect();
}

// ============================================================================
// FEED-FORWARD NETWORK
// ============================================================================

struct FeedForward {
    w1: Tensor[f32, (D_MODEL, D_FF)],
    b1: Vec<f32>,
    w2: Tensor[f32, (D_FF, D_MODEL)],
    b2: Vec<f32>,
}

fn create_ff() -> FeedForward {
    let scale1 = (2.0 / D_MODEL as f32).sqrt();
    let scale2 = (2.0 / D_FF as f32).sqrt();

    return FeedForward {
        w1: random_tensor((D_MODEL, D_FF), scale1),
        b1: vec![0.0; D_FF],
        w2: random_tensor((D_FF, D_MODEL), scale2),
        b2: vec![0.0; D_MODEL],
    };
}

fn ff_forward(ff: &FeedForward, input: &Vec<f32>) -> Vec<f32> {
    // First linear + GELU
    let mut hidden = linear(input, &ff.w1);
    for i in 0..D_FF {
        hidden[i] += ff.b1[i];
        hidden[i] = gelu(hidden[i]);
    }

    // Second linear
    let mut output = linear(&hidden, &ff.w2);
    for i in 0..D_MODEL {
        output[i] += ff.b2[i];
    }

    return output;
}

// GELU activation
fn gelu(x: f32) -> f32 {
    return 0.5 * x * (1.0 + math.tanh(0.7978845608 * (x + 0.044715 * x * x * x)));
}

// ============================================================================
// TRANSFORMER LAYER
// ============================================================================

struct TransformerLayer {
    attention: MultiHeadAttention,
    ff: FeedForward,
    ln1_gamma: Vec<f32>,
    ln1_beta: Vec<f32>,
    ln2_gamma: Vec<f32>,
    ln2_beta: Vec<f32>,
}

fn create_layer() -> TransformerLayer {
    return TransformerLayer {
        attention: create_mha(),
        ff: create_ff(),
        ln1_gamma: vec![1.0; D_MODEL],
        ln1_beta: vec![0.0; D_MODEL],
        ln2_gamma: vec![1.0; D_MODEL],
        ln2_beta: vec![0.0; D_MODEL],
    };
}

fn layer_forward(layer: &TransformerLayer, input: &Vec<Vec<f32>>) -> Vec<Vec<f32>> {
    let num_moves = input.len();

    // Self-attention
    let attn_out = attention_forward(&layer.attention, input);

    // Add & Norm
    let mut hidden = Vec.new();
    for i in 0..num_moves {
        let sum = add_vectors(&input[i], &attn_out[i]);
        hidden.push(layer_norm(&sum, &layer.ln1_gamma, &layer.ln1_beta));
    }

    // Feed-forward
    let ff_out: Vec<Vec<f32>> = hidden.iter()
        .map(|h| ff_forward(&layer.ff, h))
        .collect();

    // Add & Norm
    let mut output = Vec.new();
    for i in 0..num_moves {
        let sum = add_vectors(&hidden[i], &ff_out[i]);
        output.push(layer_norm(&sum, &layer.ln2_gamma, &layer.ln2_beta));
    }

    return output;
}

// ============================================================================
// FULL TRANSFORMER HEAD
// ============================================================================

struct TransformerHead {
    layers: Vec<TransformerLayer>,
    output_proj: Vec<f32>,   // [D_MODEL] -> scalar
    output_bias: f32,
}

fn create_transformer() -> TransformerHead {
    let mut layers = Vec.new();
    for _ in 0..N_LAYERS {
        layers.push(create_layer());
    }

    let scale = (2.0 / D_MODEL as f32).sqrt();
    return TransformerHead {
        layers: layers,
        output_proj: random_vec(D_MODEL, scale),
        output_bias: 0.0,
    };
}

// Rerank moves using transformer
// Input: NNUE accumulators for each candidate move
// Output: Reranked scores
fn rerank_moves(
    transformer: &TransformerHead,
    move_embeddings: &Vec<Vec<f32>>  // [num_moves, D_MODEL]
) -> Vec<f32> {
    if move_embeddings.is_empty() {
        return vec![];
    }

    let num_moves = move_embeddings.len();

    // Apply transformer layers
    let mut hidden = move_embeddings.clone();
    for layer in &transformer.layers {
        hidden = layer_forward(layer, &hidden);
    }

    // Project to scores
    let mut scores = Vec.with_capacity(num_moves);
    for emb in &hidden {
        let score = dot_product(emb, &transformer.output_proj) + transformer.output_bias;
        scores.push(score);
    }

    return scores;
}

// ============================================================================
// INTEGRATION WITH SEARCH
// ============================================================================

// Rerank root moves for better move ordering
fn rerank_root_moves(
    transformer: &TransformerHead,
    moves: &Vec<(Move, i32)>,       // (move, NNUE score)
    accumulators: &Vec<Vec<f32>>    // NNUE hidden states for each move
) -> Vec<(Move, f32)> {
    if accumulators.is_empty() {
        // Fallback: just convert scores
        return moves.iter()
            .map(|(m, s)| (*m, *s as f32))
            .collect();
    }

    // Get transformer scores
    let transformer_scores = rerank_moves(transformer, accumulators);

    // Combine NNUE scores with transformer scores
    // Blend: 70% NNUE + 30% transformer
    let mut result = Vec.new();
    for (i, (mv, nnue_score)) in moves.iter().enumerate() {
        let combined = 0.7 * (*nnue_score as f32) + 0.3 * transformer_scores[i] * 100.0;
        result.push((*mv, combined));
    }

    // Sort by combined score (descending)
    result.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());

    return result;
}

// ============================================================================
// HELPER FUNCTIONS
// ============================================================================

fn linear(input: &Vec<f32>, weight: &Tensor[f32, (_, _)]) -> Vec<f32> {
    let in_dim = input.len();
    let out_dim = weight.shape().1;
    let mut output = vec![0.0f32; out_dim];

    for i in 0..out_dim {
        for j in 0..in_dim {
            output[i] += input[j] * weight[(j, i)];
        }
    }

    return output;
}

fn dot_product(a: &Vec<f32>, b: &Vec<f32>) -> f32 {
    let mut sum = 0.0f32;
    for i in 0..a.len().min(b.len()) {
        sum += a[i] * b[i];
    }
    return sum;
}

fn add_vectors(a: &Vec<f32>, b: &Vec<f32>) -> Vec<f32> {
    let mut result = Vec.new();
    for i in 0..a.len() {
        result.push(a[i] + b[i]);
    }
    return result;
}

fn layer_norm(input: &Vec<f32>, gamma: &Vec<f32>, beta: &Vec<f32>) -> Vec<f32> {
    let n = input.len() as f32;

    // Compute mean
    let mean: f32 = input.iter().sum::<f32>() / n;

    // Compute variance
    let var: f32 = input.iter().map(|x| (x - mean) * (x - mean)).sum::<f32>() / n;
    let std = (var + 1e-5).sqrt();

    // Normalize and scale
    let mut output = Vec.new();
    for i in 0..input.len() {
        output.push(gamma[i] * (input[i] - mean) / std + beta[i]);
    }

    return output;
}

fn softmax_inplace(v: &mut Vec<f32>) {
    // Find max for numerical stability
    let max_val = v.iter().cloned().fold(f32::NEG_INFINITY, f32::max);

    // Compute exp and sum
    let mut sum = 0.0f32;
    for x in v.iter_mut() {
        *x = (*x - max_val).exp();
        sum += *x;
    }

    // Normalize
    for x in v.iter_mut() {
        *x /= sum;
    }
}

fn random_tensor(shape: (usize, usize), scale: f32) -> Tensor[f32, _] {
    let mut t = tensor.zeros(shape);
    for i in 0..shape.0 {
        for j in 0..shape.1 {
            t[(i, j)] = (rand.random() - 0.5) * 2.0 * scale;
        }
    }
    return t;
}

fn random_vec(size: usize, scale: f32) -> Vec<f32> {
    let mut v = Vec.with_capacity(size);
    for _ in 0..size {
        v.push((rand.random() - 0.5) * 2.0 * scale);
    }
    return v;
}
